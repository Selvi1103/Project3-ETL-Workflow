{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Key': 'sourcedataset/source1.csv', 'LastModified': datetime.datetime(2025, 3, 4, 15, 37, 10, tzinfo=tzutc()), 'ETag': '\"ce1d038c596e21cdda63fdd492ff888c\"', 'ChecksumAlgorithm': ['CRC32'], 'ChecksumType': 'FULL_OBJECT', 'Size': 109, 'StorageClass': 'STANDARD'}\n",
      "*********************************************\n",
      "{'Key': 'sourcedataset/source1.json', 'LastModified': datetime.datetime(2025, 3, 4, 15, 37, 10, tzinfo=tzutc()), 'ETag': '\"957400fea14917bedc31d2cd94b3a04b\"', 'ChecksumAlgorithm': ['CRC32'], 'ChecksumType': 'FULL_OBJECT', 'Size': 188, 'StorageClass': 'STANDARD'}\n",
      "*********************************************\n",
      "{'Key': 'sourcedataset/source1.xml', 'LastModified': datetime.datetime(2025, 3, 4, 15, 37, 10, tzinfo=tzutc()), 'ETag': '\"daab0cb0f18c3507dbab78405d718100\"', 'ChecksumAlgorithm': ['CRC32'], 'ChecksumType': 'FULL_OBJECT', 'Size': 488, 'StorageClass': 'STANDARD'}\n",
      "*********************************************\n",
      "{'Key': 'sourcedataset/source2.csv', 'LastModified': datetime.datetime(2025, 3, 4, 15, 37, 11, tzinfo=tzutc()), 'ETag': '\"ce1d038c596e21cdda63fdd492ff888c\"', 'ChecksumAlgorithm': ['CRC32'], 'ChecksumType': 'FULL_OBJECT', 'Size': 109, 'StorageClass': 'STANDARD'}\n",
      "*********************************************\n",
      "{'Key': 'sourcedataset/source2.json', 'LastModified': datetime.datetime(2025, 3, 4, 15, 37, 11, tzinfo=tzutc()), 'ETag': '\"957400fea14917bedc31d2cd94b3a04b\"', 'ChecksumAlgorithm': ['CRC32'], 'ChecksumType': 'FULL_OBJECT', 'Size': 188, 'StorageClass': 'STANDARD'}\n",
      "*********************************************\n",
      "{'Key': 'sourcedataset/source2.xml', 'LastModified': datetime.datetime(2025, 3, 4, 15, 37, 12, tzinfo=tzutc()), 'ETag': '\"daab0cb0f18c3507dbab78405d718100\"', 'ChecksumAlgorithm': ['CRC32'], 'ChecksumType': 'FULL_OBJECT', 'Size': 488, 'StorageClass': 'STANDARD'}\n",
      "*********************************************\n",
      "{'Key': 'sourcedataset/source3.csv', 'LastModified': datetime.datetime(2025, 3, 4, 15, 37, 12, tzinfo=tzutc()), 'ETag': '\"ce1d038c596e21cdda63fdd492ff888c\"', 'ChecksumAlgorithm': ['CRC32'], 'ChecksumType': 'FULL_OBJECT', 'Size': 109, 'StorageClass': 'STANDARD'}\n",
      "*********************************************\n",
      "{'Key': 'sourcedataset/source3.json', 'LastModified': datetime.datetime(2025, 3, 4, 15, 37, 12, tzinfo=tzutc()), 'ETag': '\"957400fea14917bedc31d2cd94b3a04b\"', 'ChecksumAlgorithm': ['CRC32'], 'ChecksumType': 'FULL_OBJECT', 'Size': 188, 'StorageClass': 'STANDARD'}\n",
      "*********************************************\n",
      "{'Key': 'sourcedataset/source3.xml', 'LastModified': datetime.datetime(2025, 3, 4, 15, 37, 13, tzinfo=tzutc()), 'ETag': '\"daab0cb0f18c3507dbab78405d718100\"', 'ChecksumAlgorithm': ['CRC32'], 'ChecksumType': 'FULL_OBJECT', 'Size': 488, 'StorageClass': 'STANDARD'}\n",
      "*********************************************\n",
      "['sourcedataset/source1.csv', 'sourcedataset/source2.csv', 'sourcedataset/source3.csv']\n",
      "['sourcedataset/source1.json', 'sourcedataset/source2.json', 'sourcedataset/source3.json']\n",
      "**************************************************\n",
      "['sourcedataset/source1.xml', 'sourcedataset/source2.xml', 'sourcedataset/source3.xml']\n",
      "***********************************************\n",
      "     name height  weight\n",
      "0    alex  65.78  112.99\n",
      "1    ajay  71.52  136.49\n",
      "2   alice   69.4  153.03\n",
      "3    ravi  68.22  142.34\n",
      "4     joe  67.79   144.3\n",
      "5    jack   68.7   123.3\n",
      "6     tom   69.8  141.49\n",
      "7   tracy  70.01  136.46\n",
      "8    john   67.9  112.37\n",
      "9   simon  67.90  112.37\n",
      "10  jacob  66.78  120.67\n",
      "11  cindy  66.49  127.45\n",
      "12   ivan  67.62  114.14\n",
      "     name  height  weight  HeightsinMeters  WeightsinKgs\n",
      "0    alex   65.78  112.99        20.049744     51.251360\n",
      "1    ajay   71.52  136.49        21.799296     61.910772\n",
      "2   alice   69.40  153.03        21.153120     69.413184\n",
      "3    ravi   68.22  142.34        20.793456     64.564285\n",
      "4     joe   67.79  144.30        20.662392     65.453326\n",
      "5    jack   68.70  123.30        20.939760     55.927894\n",
      "6     tom   69.80  141.49        21.275040     64.178732\n",
      "7   tracy   70.01  136.46        21.339048     61.897164\n",
      "8    john   67.90  112.37        20.695920     50.970133\n",
      "9   simon   67.90  112.37        20.695920     50.970133\n",
      "10  jacob   66.78  120.67        20.354544     54.734947\n",
      "11  cindy   66.49  127.45        20.266152     57.810300\n",
      "12   ivan   67.62  114.14        20.610576     51.772991\n",
      "File uploaded successfully to my-etl-project-transformeddata\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob as gb\n",
    "import pandas as pd\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import json\n",
    "import xml.etree.ElementTree as et\n",
    "import boto3\n",
    "import io\n",
    "from io import StringIO\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "# logging configuration\n",
    "\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='Etllog.log', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# AWS credentials\n",
    "config_data=json.load(open(\"C:\\\\Users\\\\User\\\\iamconfig.json\"))\n",
    "access_key=config_data[\"aws_access_key_id\"]\n",
    "secret_key=config_data[\"aws_secret_access_key\"]\n",
    "\n",
    "# creating a s3_client using our credentials\n",
    "s3=boto3.client(\"s3\",aws_access_key_id=access_key,aws_secret_access_key=secret_key) \n",
    "bucket_name=\"my-etl-project-source\"\n",
    "s3_folder=\"sourcedataset\"\n",
    "def uploadfileintos3():\n",
    "    try:\n",
    "        filelist=os.listdir(\"C:\\\\Users\\\\User\\\\source1\")\n",
    "        for file in filelist:\n",
    "            localfilepath=f\"C:\\\\Users\\\\User\\\\source1\\\\{file}\"\n",
    "            s3_key=f\"{s3_folder}/{file}\"\n",
    "            response = s3.upload_file(localfilepath,bucket_name,s3_key)\n",
    "            logger.info('File uploaed into S3 bucket')\n",
    "            return response\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error in uploading file into S3:{e}')\n",
    "        raise\n",
    "\n",
    "   \n",
    "# Specify your bucket name\n",
    "bucket_name = 'my-etl-project-source'\n",
    "\n",
    "# List objects in the bucket\n",
    "response = s3.list_objects_v2(Bucket=bucket_name)\n",
    "for key in response.get('Contents', []):\n",
    "    print(key)\n",
    "    print('*********************************************')\n",
    "\n",
    "# Filter CSV files\n",
    "csv_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.csv')]\n",
    "print(csv_files)\n",
    "\n",
    "# Function to read CSV from S3\n",
    "def read_csv_from_s3(bucket, key):\n",
    "    \n",
    "    try:\n",
    "        csv_obj = s3.get_object(Bucket=bucket, Key=key)\n",
    "        body = csv_obj['Body'].read().decode('utf-8')\n",
    "        logger.info('Data extracted from csv files')\n",
    "        return pd.read_csv(StringIO(body))\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error in Extracting data from csv files:{e}')\n",
    "        raise\n",
    "\n",
    "# Function for Read and concatenate all CSV files into a single DataFrame\n",
    "def combined_csv():\n",
    "    dataframes = [read_csv_from_s3(bucket_name, key) for key in csv_files]\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    combined_df=combined_df.drop_duplicates()\n",
    "    # Remove multiple headers\n",
    "    header = combined_df.columns\n",
    "    combined_df = combined_df[~combined_df.apply(lambda row: row.equals(header), axis=1)]\n",
    "\n",
    "    # Reset index after cleaning\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "    #print(combined_df)\n",
    "    # Display the cleaned DataFrame\n",
    "    return combined_df\n",
    "\n",
    "\n",
    "# Function to read json from S3\n",
    "json_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.json')]\n",
    "print(json_files)\n",
    "print('**************************************************')\n",
    "\n",
    "# Read JSON files\n",
    "def read_json_from_s3(bucket, key):\n",
    "    for file in json_files:\n",
    "        json_obj = s3.get_object(Bucket=bucket_name, Key=file)\n",
    "        file_content = json_obj['Body'].read().decode('utf-8')\n",
    "        #print(file_content)\n",
    "        return pd.read_json(StringIO(file_content),lines=True)\n",
    "   \n",
    "def combined_json():\n",
    "    dataframes = [read_json_from_s3(bucket_name, key) for key in json_files]\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    combined_df=combined_df.drop_duplicates()\n",
    "    # Remove multiple headers\n",
    "    header = combined_df.columns\n",
    "    combined_df = combined_df[~combined_df.apply(lambda row: row.equals(header), axis=1)]\n",
    "\n",
    "    # Reset index after cleaning\n",
    "    combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Display the cleaned DataFrame\n",
    "    return(combined_df)\n",
    "\n",
    "\n",
    "# Filter XML files\n",
    "\n",
    "\n",
    "xml_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].endswith('.xml')]\n",
    "print(xml_files)\n",
    "print('***********************************************')\n",
    "\n",
    "# Function to parse XML content to DataFrame\n",
    "def parse_xml_to_df(xml_content):\n",
    "    root = ET.fromstring(xml_content)\n",
    "    data = []\n",
    "    for child in root:\n",
    "        data.append({elem.tag: elem.text for elem in child})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Read XML files and convert to DataFrame\n",
    "def combined_xml():\n",
    "    dataframes = []\n",
    "    for xml_file in xml_files:\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=xml_file)\n",
    "        xml_content = obj['Body'].read().decode('utf-8')\n",
    "        xml_df = parse_xml_to_df(xml_content)\n",
    "        dataframes.append(xml_df)\n",
    "\n",
    "    # Combine all DataFrames\n",
    "    combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "    combined_df=combined_df.drop_duplicates()\n",
    "\n",
    "    # Remove multiple headers\n",
    "    combined_df = combined_df[combined_df.apply(lambda x: not all(x == combined_df.columns), axis=1)]\n",
    "\n",
    "    # Display the final DataFrame\n",
    "    return(combined_df)\n",
    "\n",
    "def Combined_df_csv_xml_json():\n",
    "     try:\n",
    "        csv_df=combined_csv()\n",
    "        json_df=combined_json()\n",
    "        xml_df=combined_xml()\n",
    "        df=[csv_df,json_df,xml_df]\n",
    "        cdf=pd.concat(df,ignore_index=True)\n",
    "        #cdf1=cdf.drop_duplicates()\n",
    "        logger.info('All data frames combined successfully')\n",
    "        return cdf\n",
    "     except Exception as e:\n",
    "        logger.error(f'Error in combining all data farmes:{e}')\n",
    "        raise\n",
    "\n",
    "DF=Combined_df_csv_xml_json()\n",
    "print(DF)\n",
    "\n",
    "def transform_data(DF):\n",
    "    try:\n",
    "        DF['height'] = pd.to_numeric(DF['height'])\n",
    "        DF['HeightsinMeters']=DF['height']*0.3048\n",
    "        DF['weight'] = pd.to_numeric(DF['weight'])\n",
    "        DF['WeightsinKgs']=DF['weight']*0.453592\n",
    "        logger.info('Data transformed successfully')\n",
    "        return(DF)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error transforming data :{e}')\n",
    "        raise\n",
    "df1=transform_data(DF)\n",
    "print(df1)\n",
    "# dataframe to CSV file\n",
    "def load_data(DF,CSVfile):\n",
    "    try: \n",
    "        DF.to_csv(CSVfile,index=False)\n",
    "        logger.info('Data Loading is successful')\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error in Loading data :{e}')\n",
    "        raise\n",
    "def etl_process():\n",
    "        Output_path='C:/Users/User/Source1_Output/csvfile.csv'\n",
    "        data =Combined_df_csv_xml_json()\n",
    "        T_data=transform_data(data)\n",
    "        load_data(T_data,Output_path)\n",
    "        logger.info('ETL process completed successfully')\n",
    "   \n",
    "\n",
    "def upload_transformed_fileintos3():\n",
    "    try:\n",
    "        #transformed_file=os.listdir(\"C:\\\\Users\\\\User\\\\Source1_output\")\n",
    "        tbucket_name=\"my-etl-project-transformeddata\"\n",
    "        localfilepath=f\"C:\\\\Users\\\\User\\\\Source1_output\\\\csvfile.csv\"\n",
    "        s3_key=\"Csvfile.csv\"\n",
    "        response1 = s3.upload_file(localfilepath,tbucket_name,s3_key)\n",
    "        print(f\"File uploaded successfully to {tbucket_name}\")\n",
    "        logger.info('File uploaed into S3 bucket')\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error in uploading file into S3:{e}')\n",
    "        raise\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    etl_process()\n",
    "    \n",
    "    upload_transformed_fileintos3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
